---
---


@inproceedings{gomezexemplar,
  title={Exemplar-free Continual Representation Learning via Learnable Drift Compensation},
  author={Gomez-Villa, Alex and Goswami, Dipam and Wang, Kai and Bagdanov, Andrew D and Twardowski, Bartlomiej and Van De Weijer, Joost},
  booktitle={European Conference on Computer Vision (ECCV)},
  arxiv={2407.08536},
  abbr={ECCV 2024},
  preview={eccv24.png},
  abstract={Exemplar-free class-incremental learning using a backbone trained from scratch and starting from a small first task presents a sig- nificant challenge for continual representation learning. Prototype-based approaches, when continually updated, face the critical issue of semantic drift due to which the old class prototypes drift to different positions in the new feature space. Through an analysis of prototype-based continual learning, we show that forgetting is not due to diminished discriminative power of the feature extractor, and can potentially be corrected by drift compensation. To address this, we propose Learnable Drift Compensa- tion (LDC), which can effectively mitigate drift in any moving backbone, whether supervised or unsupervised. LDC is fast and straightforward to integrate on top of existing continual learning approaches. Furthermore, we showcase how LDC can be applied in combination with self-supervised CL methods, resulting in the first exemplar-free semi-supervised contin- ual learning approach. We achieve state-of-the-art performance in both supervised and semi-supervised settings across multiple datasets.},
  year={2024}
}

@inproceedings{goswami2024calibrating,
  title={Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers},
  author={Goswami, Dipam and Twardowski, Bart{\l}omiej and Van De Weijer, Joost},
  booktitle={CVPR Workshops - CLVISION},
  abstract={Few-shot class-incremental learning (FSCIL) aims to adapt the model to new classes from very few data (5 sam- ples) without forgetting the previously learned classes. Re- cent works in many-shot CIL (MSCIL) (using all available training data) exploited pre-trained models to reduce for- getting and achieve better plasticity. In a similar fashion, we use ViT models pre-trained on large-scale datasets for few-shot settings, which face the critical issue of low plas- ticity. FSCIL methods start with a many-shot first task to learn a very good feature extractor and then move to the few-shot setting from the second task onwards. While the fo- cus of most recent studies is on how to learn the many-shot first task so that the model generalizes to all future few-shot tasks, we explore in this work how to better model the few- shot data using pre-trained models, irrespective of how the first task is trained. Inspired by recent works in MSCIL, we explore how using higher-order feature statistics can in- fluence the classification of few-shot classes. We identify the main challenge of obtaining a good covariance matrix from few-shot data and propose to calibrate the covariance matrix for new classes based on semantic similarity to the many-shot base classes. Using the calibrated feature statis- tics in combination with existing methods significantly im- proves few-shot continual classification on several FSCIL benchmarks.},
  abbr={CVPRW 2024},
  arxiv={2404.06622},
  preview={cvprw24.png},
  code={https://github.com/dipamgoswami/FSCIL-Calibration},
  year={2024}
}

@inproceedings{goswami2024resurrecting,
  title={Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning},
  author={Goswami, Dipam and Soutif-Cormerais, Albin and Liu, Yuyang and Kamath, Sandesh and Twardowski, Bart{\l}omiej and Van De Weijer, Joost},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  abstract={Continual learning methods are known to suffer from catastrophic forgetting, a phenomenon that is particularly hard to counter for methods that do not store exemplars of previous tasks. Therefore, to reduce potential drift in the feature extractor, existing exemplar-free methods are typ- ically evaluated in settings where the first task is signif- icantly larger than subsequent tasks. Their performance drops drastically in more challenging settings starting with a smaller first task. To address this problem of feature drift estimation for exemplar-free methods, we propose to adver- sarially perturb the current samples such that their embed- dings are close to the old class prototypes in the old model embedding space. We then estimate the drift in the embed- ding space from the old to the new model using the per- turbed images and compensate the prototypes accordingly. We exploit the fact that adversarial samples are transferable from the old to the new feature space in a continual learning setting. The generation of these images is simple and com- putationally cheap. We demonstrate in our experiments that the proposed approach better tracks the movement of proto- types in embedding space and outperforms existing meth- ods on several standard continual learning benchmarks as well as on fine-grained datasets.},
  abbr={CVPR 2024},
  arxiv={2405.19074},
  preview={cvpr24.png},
  code={https://github.com/dipamgoswami/ADC},
  year={2024},
  selected={true}
}

@inproceedings{goswami2023fecam,
    title={Fe{CAM}: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning},
    author={Dipam Goswami and Yuyang Liu and Bart{\l}omiej Twardowski and Joost Van De Weijer},
    booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
    abstract={Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is better than previous attempts at sampling features from normal distributions and training a linear classifier. Unlike existing methods, our approach generalizes to both many- and few-shot CIL settings, as well as to domain-incremental settings. Interestingly, without updating the backbone network, our method obtains state-of-the-art results on several standard continual learning benchmarks.},
    abbr={NeurIPS 2023},
    arxiv={2309.14062},
    preview={neurips23.png},
    code={https://github.com/dipamgoswami/FeCAM},
    year={2023},
    selected={true}
}

@inproceedings{liu2023augmented,
  title={Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection},
  author={Liu, Yuyang and Cong, Yang and Goswami, Dipam and Liu, Xialei and Van De Weijer, Joost},
  booktitle={International Conference on Computer Vision (ICCV)},
  abstract={In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model.},
  abbr={ICCV 2023},
  arxiv={2307.12427},
  preview={iccv23.png},
  year={2023}
}

@inproceedings{goswami2023attribution,
  title={Attribution-aware weight transfer: A warm-start initialization for class-incremental semantic segmentation},
  author={Goswami, Dipam and Schuster, Ren{\'e} and Van De Weijer, Joost and Stricker, Didier},
  booktitle={Winter Conference on Applications of Computer Vision (WACV)},
  abstract={In class-incremental semantic segmentation (CISS), deep learning architectures suffer from the critical problems of catastrophic forgetting and semantic background shift. Although recent works focused on these issues, existing classifier initialization methods do not address the background shift problem and assign the same initialization weights to both background and new foreground class classifiers. We propose to address the background shift with a novel classifier initialization method which employs gradient-based attribution to identify the most relevant weights for new classes from the classifier's weights for the previous background and transfers these weights to the new classifier. This warm-start weight initialization provides a general solution applicable to several CISS methods. Furthermore, it accelerates learning of new classes while mitigating forgetting. Our experiments demonstrate significant improvement in mIoU compared to the state-of-the-art CISS methods on the Pascal-VOC 2012, ADE20K and Cityscapes datasets.},
  abbr={WACV 2023},
  arxiv={2210.07207},
  preview={wacv24.png},
  code={https://github.com/dfki-av/AWT-for-CISS},
  year={2023}
}

@inproceedings{aggrawal2023bounding,
  title={Bounding Box Priors for Cell Detection with Point Annotations},
  author={Aggrawal, Hari Om and Goswami, Dipam and Agarwal, Vinti},
  booktitle={IEEE 20th International Symposium on Biomedical Imaging (ISBI)},
  abbr={ISBI 2023},
  abstract={The size of an individual cell type, such as a red blood cell, does not vary much among humans. We use this knowledge as a prior for classifying and detecting cells in images with only a few ground truth bounding box annotations, while most of the cells are annotated with points. This setting leads to weakly semi-supervised learning. We propose re- placing points with either stochastic (ST) boxes or bounding box predictions during the training process. The proposed “mean-IOU” ST box maximizes the overlap with all the boxes belonging to the sample space with a class-specific ap- proximated prior probability distribution of bounding boxes. Our method trains with both box- and point-labelled im- ages in conjunction, unlike the existing methods, which train first with box- and then point-labelled images. In the most challenging setting, when only 5% images are box-labelled, quantitative experiments on a urine dataset show that our one- stage method outperforms two-stage methods by 5.56 mAP. Furthermore, we suggest an approach that partially answers “how many box-labelled annotations are necessary?” before training a machine learning model.},
  arxiv={2211.06104},
  preview={isbi23.png},
  year={2023}
}

@article{shekhawat2021graph,
  title={Graph-based approach for enumerating floorplans based on users specifications},
  author={Shekhawat, Krishnendra and Jain, Rahil N and Bisht, Sumit and Kondaveeti, Aishwarya and Goswami, Dipam},
  journal={AI EDAM},
  abbr={AI EDAM, Cambridge University Press},
  year={2021},
  preview={floor-plan.png},
  publisher={Cambridge University Press}
}

@article{goswami2021urine,
  title={Urine microscopic image dataset},
  author={Goswami, Dipam and Aggrawal, Hari Om and Gupta, Rajiv and Agarwal, Vinti},
  journal={arXiv preprint arXiv:2111.10374},
  abbr={arxiv},
  preview={cells.png},
  code={https://github.com/dipamgoswami/UMID-Urine-Microscopic-Image-Dataset},
  year={2021}
}
